{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data from text files to  numpy arrays\n",
    "- affirmative      - 0\n",
    "- conditional      - 1\n",
    "- doubt_question   - 2\n",
    "- emphasis         - 3\n",
    "- negative         - 4\n",
    "- relative         - 5\n",
    "- topics           - 6\n",
    "- wh_question      - 7\n",
    "- yn_question      - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_list=['affirmative','conditional','doubt_question','emphasis','negative','relative','topics','wh_question','yn_question']\n",
    "\n",
    "def read_data(index):\n",
    "    '''Takes index indexing the files_list list and reads the data from corresponding text files and returns it\n",
    "        Also removes first row and first column from the data as part of first pre processing step\n",
    "        Concatenates data from two persons a and b'''\n",
    "    index=int(index)\n",
    "    x_a = np.loadtxt('grammatical_facial_expression/a_'+files_list[index]+'_datapoints.txt', skiprows=1)\n",
    "    x_b = np.loadtxt('grammatical_facial_expression/b_'+files_list[index]+'_datapoints.txt', skiprows=1)\n",
    "    y_a = np.loadtxt('grammatical_facial_expression/a_'+files_list[index]+'_targets.txt')\n",
    "    y_b = np.loadtxt('grammatical_facial_expression/b_'+files_list[index]+'_targets.txt')\n",
    "    x = np.concatenate((x_a, x_b))\n",
    "    y = np.concatenate((y_a, y_b))\n",
    "    return x[:,1:],y \n",
    "\n",
    "def read_data_one(index, name):\n",
    "    '''Takes index indexing the files_list list and reads the data from corresponding text files and returns it\n",
    "        Also removes first row and first column from the data as part of first pre processing step\n",
    "        Reads data only from one person name taken as argument a/b '''\n",
    "    index=int(index)\n",
    "    x_ = np.loadtxt('grammatical_facial_expression/'+name+'_'+files_list[index]+'_datapoints.txt', skiprows=1)\n",
    "    y_ = np.loadtxt('grammatical_facial_expression/'+name+'_'+files_list[index]+'_targets.txt')\n",
    "    return x_[:,1:],y_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Making both the classes of equal size for handling class imbalance problem\n",
    "- Shuffling the data to avoid any further problems\n",
    "- Converting y-labels as one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "    ''' Normalizes all features of the data\n",
    "        Second step of data preprocessing'''\n",
    "    size=np.shape(x)[1]\n",
    "    mean=np.mean(x,axis=0)\n",
    "    sigma=np.std(x,axis=0)\n",
    "    norm_x = (x-mean)/sigma\n",
    "    return norm_x\n",
    "\n",
    "def split_data(x, y, ratio):\n",
    "    '''Splits the dataset according to the ratio given.\n",
    "        Shuffles the data using random shuffling before splitting\n",
    "        Converts the y labels as one hot vectors'''\n",
    "    y=y[:,np.newaxis]\n",
    "    X=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(X)\n",
    "    train_size = int(np.shape(X)[0] * ratio)\n",
    "    y_train = np.zeros((train_size,2))\n",
    "    y_test = np.zeros((X.shape[0]-train_size, 2))\n",
    "    y1=X[:train_size,-1]\n",
    "    y2=X[train_size:,-1]\n",
    "    y_train[y1==0,0]=1\n",
    "    y_train[y1==1,1]=1\n",
    "    y_test[y2==0,0]=1\n",
    "    y_test[y2==1,1]=1\n",
    "    return X[:train_size,:-1], y_train, X[train_size:,:-1 ], y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data, y_data = read_data_one(0, 'a') #read data 5->relative #person A\n",
    "x_data = normalize_data(x_data) #normalize\n",
    "#np.savetxt('a_relative.csv',x_data,delimiter=',')\n",
    "x_data = x_data*100\n",
    "x_train, y_train, x_test, y_test = split_data(x_data, y_data, 0.8) #test-train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining required placeholders and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_size = np.shape(x_train)[1]\n",
    "# Defining Placeholders for input data and labels\n",
    "X = tf.placeholder(tf.float32, (None, x_size), name='X')\n",
    "Y = tf.placeholder(tf.float32, (None, 2), name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining variables for weights of NN\n",
    "w1 = tf.Variable(tf.random_normal((300,1)), name=\"w1\") # Weights for hidden layer 1\n",
    "b1 = tf.Variable(tf.reshape(tf.random_normal((100,1)),[100]), name=\"b1\")     # Bias for hidden layer 1\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal((100,1)), name='w2') # Weights for hidden layer 2\n",
    "b2=tf.Variable(tf.reshape(tf.random_normal((10,1)),[10]), name='b2')        # Bias for hidden layer 2\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal((10,2)),name='w3')  # Weights for output layer\n",
    "b3 = tf.Variable(tf.random_normal((1,2)), name='b3')  # Bias for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining nodes connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Layer\n",
    "\n",
    "i=0\n",
    "z_i = tf.nn.relu(tf.add(tf.matmul(X[:,i:i+3], w1[i:i+3,:]), b1[int(i/3)]), name='layer_1') #first node of hidden layer 1\n",
    "first_layer = z_i\n",
    "\n",
    "for i in range(3,300,3):\n",
    "    z_i = tf.nn.relu(tf.add(tf.matmul(X[:,i:i+3], w1[i:i+3,:]), b1[int(i/3)])) #Remaining nodes of hidden layer 2\n",
    "    first_layer = tf.concat((first_layer, z_i), axis=1)\n",
    "#print(w1)\n",
    "#print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second Layer\n",
    "\n",
    "#Left Eye\n",
    "z21 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,0:8], w2[0:8,:]), b2[0]), name='layer_2')\n",
    "second_layer = z21\n",
    "\n",
    "#Right Eye\n",
    "z22 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,8:16], w2[8:16,:]), b2[1]))\n",
    "second_layer = tf.concat((second_layer, z22), axis=1)\n",
    "\n",
    "#Left Eyebrow\n",
    "z23 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,16:26], w2[16:26,:]), b2[2]))\n",
    "second_layer = tf.concat((second_layer, z23), axis=1)\n",
    "\n",
    "#Right Eyebrow\n",
    "z24 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,26:36], w2[26:36,:]), b2[3]))\n",
    "second_layer = tf.concat((second_layer, z24), axis=1)\n",
    "\n",
    "#Nose\n",
    "z25 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,36:48], w2[36:48,:]), b2[4]))\n",
    "second_layer = tf.concat((second_layer, z25), axis=1)\n",
    "\n",
    "#Mouth\n",
    "z26 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,48:68], w2[48:68,:]), b2[5]))\n",
    "second_layer = tf.concat((second_layer, z26), axis=1)\n",
    "\n",
    "#Face Contour\n",
    "z27 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,68:87], w2[68:87,:]), b2[6]))\n",
    "second_layer = tf.concat((second_layer, z27), axis=1)\n",
    "\n",
    "# Left, right iris + nose tip\n",
    "z28 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,87:90], w2[87:90,:]), b2[7]))\n",
    "second_layer = tf.concat((second_layer, z28), axis=1)\n",
    "\n",
    "#Line above left eyebrow\n",
    "z29 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,90:95], w2[90:95,:]), b2[8]))\n",
    "second_layer = tf.concat((second_layer, z29), axis=1)\n",
    "\n",
    "#Left Eye\n",
    "z210 = tf.nn.relu(tf.add(tf.matmul(first_layer[:,95:], w2[95:,:]), b2[9]))\n",
    "second_layer = tf.concat((second_layer, z210), axis=1)\n",
    "\n",
    "#print(w2)\n",
    "#print(second_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output layer\n",
    "\n",
    "z3 = tf.add(tf.matmul(second_layer, w3), b3)\n",
    "out_weights = tf.reshape(w3, [1,20])\n",
    "\n",
    "output=tf.nn.softmax(z3, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Cost and Training step tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast( tf.equal(tf.argmax(Y,1), tf.argmax(output,1)),tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function\n",
    "\n",
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output), name='cost')\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 accuracy: 0.803298 loss: 437.1862\n",
      "epoch: 20 accuracy: 0.8351001 loss: 406.20282\n",
      "epoch: 40 accuracy: 0.8515901 loss: 391.93997\n",
      "epoch: 80 accuracy: 0.85276794 loss: 390.96027\n",
      "epoch: 160 accuracy: 0.85276794 loss: 390.96\n",
      "epoch: 320 accuracy: 0.85276794 loss: 390.95984\n",
      "epoch: 640 accuracy: 0.85276794 loss: 390.95975\n",
      "epoch: 1280 accuracy: 0.85276794 loss: 390.95966\n"
     ]
    }
   ],
   "source": [
    "sess =  tf.Session() \n",
    "writer = tf.summary.FileWriter('./project_graph', sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "checkpoint=10\n",
    "for e in range(1300):\n",
    "    _,l = sess.run([train_step, cost], feed_dict={X:x_train, Y:y_train})\n",
    "    if(e==checkpoint):\n",
    "        acc = sess.run(accuracy, feed_dict={X:x_train, Y:y_train})\n",
    "        print('epoch: '+str(e)+' accuracy: ' +str(acc) + ' loss: '+str(l))\n",
    "        checkpoint=checkpoint*2\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8309859\n"
     ]
    }
   ],
   "source": [
    "acc_test = sess.run(accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print('Test Accuracy: '+ str(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
